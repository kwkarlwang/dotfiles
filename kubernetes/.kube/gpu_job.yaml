apiVersion: batch/v1
kind: Job
metadata:
  name: karl-job-8gpu # specify your job name here
spec:
  template:
    spec:
      #   affinity:
      #     nodeAffinity:
      #       requiredDuringSchedulingIgnoredDuringExecution:
      #         nodeSelectorTerms:
      #           - matchExpressions:
      #               - key: gpu-type
      #                 operator: In
      #                 values:
      #                   - "TITAN RTX"
      #                   - "V100"
      #                   - "T4"
      #                   - "3090Ti"
      #                   - "A100"
      #                   - "8000"
      #                   - "A40"

      containers:
        - name: gpu-container
          image: gitlab-registry.nautilus.optiputer.net/jiacheng/docker-images:torch17v4_jiacheng
          # image: gitlab-registry.nrp-nautilus.io/kwkarlwang/nautilus:latest
          command:
            - bash
            - -c
            - "conda init && source ~/.bashrc && nvidia-smi && rm -rf /root/.cache && && source /data/karl/setup.sh && cd /data/karl/AdaBins-1 && python train.py args_train_nyu.txt"
          #args:
          #- ""

          volumeMounts:
            - mountPath: /data
              name: data
            - mountPath: /dev/shm
              name: dshm
          resources:
            limits:
              memory: 8Gi
              nvidia.com/gpu: "8"
              cpu: "12"
            requests:
              memory: 8Gi
              nvidia.com/gpu: "8"
              cpu: "12"
      restartPolicy: Never
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: storage
        - name: dshm
          emptyDir:
            medium: Memory
  backoffLimit: 1
